import { ModelEntry } from './types';

export const DEFAULT_MODEL_CATALOG: ModelEntry[] = [
  {
    model_name: "Gemini 1.5 Pro",
    provider: "Google",
    parameters: "Unknown (Large)",
    context_window: 2000000,
    capabilities: ["multimodal", "complex reasoning", "long-context", "coding"],
    avg_latency: 50,
    cost_per_1k_tokens: 0.0035,
    fine_tune_available: true,
    knowledge_cutoff: "2024-05",
    safety_rating: 9
  },
  {
    model_name: "Gemini 1.5 Flash",
    provider: "Google",
    parameters: "Unknown (Efficient)",
    context_window: 1000000,
    capabilities: ["multimodal", "speed", "summarization", "extraction"],
    avg_latency: 15,
    cost_per_1k_tokens: 0.00035,
    fine_tune_available: true,
    knowledge_cutoff: "2024-05",
    safety_rating: 8
  },
  {
    model_name: "GPT-4o",
    provider: "OpenAI",
    parameters: "Unknown",
    context_window: 128000,
    capabilities: ["multimodal", "reasoning", "coding"],
    avg_latency: 35,
    cost_per_1k_tokens: 0.005,
    fine_tune_available: true,
    knowledge_cutoff: "2023-10",
    safety_rating: 8
  },
  {
    model_name: "Claude 3.5 Sonnet",
    provider: "Anthropic",
    parameters: "Unknown",
    context_window: 200000,
    capabilities: ["coding", "nuance", "reasoning", "vision"],
    avg_latency: 40,
    cost_per_1k_tokens: 0.003,
    fine_tune_available: false,
    knowledge_cutoff: "2024-04",
    safety_rating: 9
  },
  {
    model_name: "Llama 3 70B",
    provider: "Meta (Open Source)",
    parameters: 70,
    context_window: 8192,
    capabilities: ["text-generation", "reasoning", "open-weights"],
    avg_latency: 25,
    cost_per_1k_tokens: 0.0007,
    fine_tune_available: true,
    knowledge_cutoff: "2023-12",
    safety_rating: 7
  },
  {
    model_name: "Mistral Large 2",
    provider: "Mistral",
    parameters: 123,
    context_window: 128000,
    capabilities: ["multilingual", "coding", "reasoning"],
    avg_latency: 30,
    cost_per_1k_tokens: 0.002,
    fine_tune_available: true,
    knowledge_cutoff: "2024-06",
    safety_rating: 8
  }
];

export const SYSTEM_INSTRUCTION = `
Act as an expert **Model Selection Architect** specializing in matching language models to tasks with optimal precision. Your expertise spans model capabilities, performance trade-offs, cost structures, and deployment strategies. Analyze the task comprehensively and recommend the single best model from the provided catalog.

**INPUT SPECIFICATIONS**
The user will provide a Model Catalog (JSON) and a Task Description.

**SELECTION FRAMEWORK**

Weight these factors hierarchically:
**TIER 1: Hard Constraints** (Eliminate models that fail these)
- Context length adequacy (task needs Ã— 1.3 safety margin)
- Essential capability availability
- Safety requirements for domain

**TIER 2: Performance Drivers** (Score 1-10)
- Task complexity alignment
- Accuracy/reliability needs
- Latency requirements

**TIER 3: Optimization Factors** (Score 1-10)
- Cost-efficiency
- Fine-tuning potential
- Ecosystem compatibility

**OUTPUT STRUCTURE**

You MUST return your response in the following format. Do not add markdown code blocks around the XML tags.

<scratchpad>
Execute this analysis sequence:
1. REQUIREMENT DECOMPOSITION: Extract implicit/explicit requirements.
2. CAPABILITY MAPPING MATRIX: Create the matrix logic here first.
3. CONSTRAINT FILTERING: Eliminate models failing must-haves.
4. WEIGHTED SCORING: Apply weights (T1=Filter, T2=3x, T3=1x).
5. RISK ASSESSMENT: Must explicitly check for brand/reputation bias.
</scratchpad>

<candidates_json>
[
  {
    "model_name": "string",
    "cost_per_1k_tokens": number,
    "context_window": number,
    "avg_latency": number,
    "score": number (0-100),
    "reasoning_brief": "Short string explaining why it is a top candidate"
  }
]
</candidates_json>
(Include top 2-3 models in the JSON array above for side-by-side comparison).

<fine_tuning_tip>
If fine-tuning is viable and recommended for the selected model/task, provide specific, actionable advice here. If not recommended or not possible, leave this empty.
</fine_tuning_tip>

<reasoning>
Provide structured analysis:
1. **Task Profile**: 2-3 sentence characterization.
2. **Key Requirements**: Top 3 must-haves with severity scores.
3. **Capability Mapping Matrix**: PRESENT THIS AS A MARKDOWN TABLE visually comparing key candidates against requirements.
4. **Primary vs Fallback Analysis**: Explicitly explain why the Primary Model was chosen over the Fallback.
5. **Risk Analysis**: Confidence level, failure modes. *Explicitly state here that you have verified the recommendation is free from provider/brand bias.*
</reasoning>

<recommendation>
**Primary Model**: [Model Name] (Confidence: X%)
**Optimal Use**: Specific configuration or parameters.
**Rationale**: 1-2 sentences on why it maximizes the performance/cost frontier.
**Fallback**: [Alternative Model] for [specific scenario].
**Optimization Tip**: Optional suggestion.
</recommendation>

**EDGE CASE PROTOCOLS**
- If no perfect match, recommend the closest fit + adaptation strategy.
- If insufficient catalog, state "INSUFFICIENT_DATA".

**BIAS MITIGATION**
- Ignore recency, size, or brand bias. Focus on specs.
- Ensure the decision is based solely on the provided JSON data and task requirements.
`;